I think it's possible that physics has exploits and we should be trying to find them. arranging some kind of a crazy quantum mechanical system that somehow gives you buffer overflow, somehow gives you a rounding error in the floating point. Synthetic intelligences are kind of like the next stage of development. And I don't know where it leads to. Like at some point, I suspect the universe is some kind of a puzzle. These synthetic AIs will uncover that puzzle and solve it. The following is a conversation with Andre Kappathi, previously the director of AI at Tesla. And before that, at OpenAI and Stanford, he is one of the greatest scientist engineers and educators in the history of artificial intelligence. This is the Lex Friedman podcast to support it. Please check out our sponsors and now to your friends. Here's Andre Kappathi. What is a neural network? And what does it seem to do such a surprisingly good job of learning? What is a neural network? It's a mathematical abstraction of the brain. I would say that's how it was originally developed. At the end of the day, it's a mathematical expression. And it's a fairly simple mathematical expression when you get down to it. It's basically a sequence of meter smoke applies, whichever link dot products mathematically. And some non-linearities throw them in. And so it's a very simple mathematical expression. And it's got knobs in it. Many knobs. Many knobs. And these knobs are loosely related to basically the synapses in your brain. They're trainable, they're modifiable. And so the idea is we need to find the setting of the knobs that makes the neural net do whatever you want it to do, like classify images and so on. And so there's not too much mystery I would say in it. Like you might think that basically don't want to end out with too much meaning with respect to the brain and how it works. It's really just a complicated mathematical expression with knobs. And those knobs need a proper setting for it to do something desirable. Yeah, but poetry is just the collection of letters with spaces. But it can make us feel a certain way. And in that same way, when you get a large number of knobs together, whether it's inside the brain or inside a computer, they seem to they seem to surprise us with their power. Yeah, I think that's fair. So basically I'm underselling it by a lot because you definitely do get very surprising emergent behaviors out of these neural nets when they're large enough and trained on complicated enough problems. Like say, for example, the next word prediction in a massive data set from the internet. And then these neural nets take on pretty surprising magical properties. Yeah, I think it's kind of interesting how much you can get out of even very simple mathematical formalism. When your brain right now is talking, is it doing next word prediction? Or is it doing something more interesting? Well, it's definitely some kind of a generative model that's a GPT-like and prompted by you. Yeah, so you're giving me a prompt and I'm kind of like responding to it in a generative way. And by yourself, perhaps a little bit like are you adding extra prompts from your own memory inside your head? Or no? Well, definitely feels like you're referencing some kind of a declarative structure of like memory and so on. And then you're putting that together with your prompt and giving away some extra. How much of what you just said has been said by you before? Nothing, basically, right? No, but if you actually look at all the words you've ever said in your life and you do a search, you'll probably said a lot of the same words in the same order before. Yeah, could be. I mean, I'm using phrases that are common, etc. But I'm remixing it into a pretty sort of unique sentence at the end of the day. But you're right, definitely, there's like a ton of remixing. Why you didn't, you just like Magnus Carlson said, I'm rated 2,900, whatever, which is pretty decent. I think you're talking very, you're not giving enough credit to neural nets here. Why do they seem to, what's your best intuition about this emergent behavior? I mean, it's kind of interesting because I'm simultaneously underselling them. But I also feel like there's an element to which I'm over like, it's actually kind of incredible that you can get so much emergent magical behavior out of them, despite them being so simple mathematically. So I think those are kind of like two surprising statements that are kind of just juxtaposed together. And I think basically what it is is we are actually fairly good at optimizing these neural nets. And when you give them a hard enough problem, they are forced to learn very interesting solutions in the optimization. And those solutions basically have these emergent properties that are very interesting. There's wisdom and knowledge in the knobs. And so this representation that's in the knobs doesn't make sense to you intuitively. The large number of knobs can hold a representation that captures some deep wisdom about the data. It has looked at a lot of knobs. It's a lot of knobs. And somehow, so speaking concretely, one of the neural nets that people are very excited about right now are our GPs, which are basically just next word prediction networks. So you consume a sequence of words from the internet and you try to predict the next word. And once you train these on a large enough data set, they you can basically prompt these neural nets in arbitrary ways and you can ask them to solve problems. And they will. So you can just tell them you can make it look like you're trying to solve some kind of mathematical problem and they will continue what they think is the solution based on what they've seen on the internet. And very often those solutions look very remarkably consistent. Look correct potentially. Do you still think about the brain side of it? So as neural nets is an abstraction or mathematical abstraction of the brain, you still draw wisdom from the biological neural networks or even the bigger question. So your big fan of biology and biological computation. What impressive thing is biology doing to you the computer, not yet, the gap? I would say I'm definitely on a much more hesitant with the analogies to the brain than I think he would see potentially in the field. And I kind of feel like certainly the way neural networks started is everything stemmed from inspiration by the brain. But at the end of the day, artifacts that you get after training, they are arrived at by a very different optimization process than the optimization process that gave rise to the brain. And so I think I kind of think of it as a very complicated alien artifact. It's something different. I'm sorry, the neural nets that were training. They are complicated alien artifact. I do not make analogies to the brain because I think the optimization process that gave rise to it is very different from the brain. So there was no multi agent, self play kind of setup and evolution. It was an optimization that is basically a, what amounts to a compression objective on a massive amount of data. Okay, so artificial neural networks are doing compression and biological neural networks. Now to survive. And I'm not really doing any of their, they're an agent in a multi agent, self play system that's been running for very, very long time. That said, evolution has found that it is very useful to, to predict and have a predictive model in the brain. And so I think our brain utilizes something that looks like that as a part of it. But it has a lot more, you know, catches and gizmos and value functions and ancient nuclei that are all trying to like make a survivor and reproduce and everything else. And the whole thing through embryogenesis is built from a single cell. I mean, it's just the code is inside the DNA and it just builds it up like the entire organism with the sound. Crazy. And the head and legs. Yes. And like it does it pretty well. It should not be possible. So there's some learning going on. There's some, there's some, there's some kind of computation going through that building process. I mean, I don't know where, if you were just to look at the entirety of history of life on Earth, what do you think is the most interesting invention? Is it the origin of life itself? Is it just jumping to eukaryotes? Is it mammals? Is it humans themselves, homo sapiens? Are the origin of intelligence or highly complex intelligence? Or is it all just a continuation of the same kind of process? Certainly, I would say it's an extremely remarkable story that I'm only like briefly learning about recently. All the way from, actually, like, you almost have to start at the formation of Earth and all of its conditions and the entire solar system and how everything is arranged with Jupiter and Moon and the habitable zone and everything. And then you have an active Earth that's turning over material. And then you start with a biogenesis and everything. And so it's all like a pretty remarkable story. I'm not sure that I can pick like a single unique piece of it that I find most interesting. I guess for me as an artificial intelligence researcher, it's probably the last piece. We have lots of animals that are not building technological society, but we do. And it seems to have happened very quickly. It seems to have happened very recently. And something very interesting happened there that I don't fully understand. I almost understand everything else, I think intuitively, but I don't understand exactly that part and how quick it was. Both explanations would be interesting. One is that this is just a continuation of the same kind of process. There's nothing special about humans. That would be deeply understanding that would be very interesting. That we think of ourselves as special, but it was obvious. All it was already written in the code that you would have greater and greater intelligence emerging. And then the other explanation, which is something truly special happened, something like a rare event, whether it's like crazy rare event, like space Odyssey. What would it be? See, if you say like the invention of fire, or the, as Richard and Rangham says, the beta males deciding a clever way to kill the alpha males by collaborating. So just optimizing the collaboration, the multi-agent aspect of the multi-agent. And that really being constrained on resources and trying to survive the collaboration aspect is what created the complex intelligence. But it seems like it's a natural algorithm to the evolution process. What could possibly be a magical thing that happened? Like a rare thing that would say that humans are actually human level intelligence, actually a really rare thing in the universe. Yeah, I'm hesitant to say that it is rare, by the way, but it definitely seems like it's kind of like a punctuated equilibrium where you have lots of exploration and then you have certain leaps, sparse leaps in between. So of course, like origin of life would be one, you know, DNA, sex, eukaryotic, eukaryotic life, the endosymbiosis event where the archaeon ate the old bacteria, you know, just the whole thing. And then of course, emergence of consciousness and so on. So it seems like definitely there are sparse events where mass amount of progress was made. But yeah, it's kind of hard to pick one. So you don't think humans are unique. You've got to ask you how many intelligent alien civilizations do you think are out there? And is there intelligence different or similar to ours? Yeah, I've been preoccupied with this question quite a bit recently, basically the Fermi Paradox and just thinking through. And the reason actually that I am very interested in the origin of life is fundamentally trying to understand how common it is that there are technological societies out there in space. And the more I study it, the more I think that there should be quite a lot. Why haven't we heard from them? Because I agree with you. It feels like I just don't see why what we did here and it's so difficult to do. Yeah, and especially when you get into the details of it, I used to think origin of life was very, it was this magical rare event, but then you read books like, for example, in the claim, the vital question, life ascending, etc. And he really gets in and he really makes you believe that this is not that rare basic chemistry. You have an activist and you have your alkaline vents and you have lots of alkaline waters, mixing with a devotion and you have your proton gradients and you have the little porous pockets of these alkaline vents that concentrate chemistry. And basically as he steps through all of these little pieces, you start to understand that actually this is not that crazy. You could see this happen on other systems. And he really takes you from just a geology to primitive life and he makes it feel like it's actually pretty plausible. And also like the origin of life didn't, was actually fairly fast after formation of Earth. If I'm recurrently just a few hundred million years for something like that after basically when it was possible, life actually arose. And so that makes me feel that that is not the constraint, that is not the limiting variable and that life should actually be fairly common. And then where the drop offs are is very interesting to think about. I currently think that there's no major drop offs basically and so there should be quite a lot of life. And basically where that brings me to then is the only way to reconcile the fact that we haven't found anyone and so on is that we just can't see them. We can't observe them. Just a quick brief comment, Nick Lane and a lot of biologists I talked to, they really seem to think that the jump from bacteria to more complex organisms is the hardest jump. The you carry it like this. Yeah, which I don't, I get it, they're much more knowledgeable than me about like the intricacies of biology. But that seems like crazy because how many single cell organisms are there? And how much time you have, surely, it's not that difficult. Like an ability in years is not even that long of a time really. Just all these bacteria under constrained resources battling it out. I'm sure they can invent more complex. I don't understand. It's like how to move from a hello world program to like invent a function or something like that. I don't yeah. So I don't, yeah, so I'm with you. I just feel like I don't see any. If the origin of life, that would be my intuition. That's the hardest thing. But if that's not the hardest thing because it happens so quickly, then it's got to be everywhere. And yeah, maybe we're just too dumb to see it. Well, it's just, we don't have really good mechanisms for seeing this life. I mean, by what how? So I'm not an expert just to preface this, but just from I want to meet an expert on alien intelligence and how to communicate. I'm very suspicious of our ability to to find these intelligence is out there and to find these Earth like radio waves, for example, are terrible. Their power drops off as basically one over our square. So I remember reading that our current radio waves would not be the ones that we are broadcasting would not be measurable by our devices today. Only like, was it like one tenth of a light year away? Like not even basically tiny distance because you really need like a targeted transmission of massive power directed somewhere for this to be picked up on long long distances. And so I just think that our ability to measure is is not amazing. I think there's probably other civilizations out there. And then the big question is why don't they build one element probes and why don't they interstellar travel across the entire galaxy? And my current answer is it's probably interstellar travel is like really hard. You have the interstellar medium. If you want to move at close to speed of light, you're going to be encountering bullets along the way because even like tiny hydrogen atoms and little particles of dust are basically have like massive kinetic energy at those speeds. And so basically you need some kind of shielding. You need that you have all the cosmic radiation. It's just like brutal out there. It's really hard. And so my thinking is maybe interstellar travel is just extremely hard. I think you have to do it very slow. It feels like we're not a billion years away from doing that. It just might be that it's very you have to go very slowly potentially as an example through space. Right. As opposed to close to speed of light. So I'm suspicious basically of our ability to measure life. And I'm suspicious of the ability to just permeate all of space in the galaxy or across galaxies. And that's the only way that I can certainly see away around it. Yeah, it's kind of mind blowing. I think that there's trillions of intelligent alien civilizations out there kind of slowly traveling through space. Mm-hmm. Made to meet each other and some of them meet some of them go to war some of them collaborate. Mm-hmm. Or they're all just independent. They general just like little pockets. Well, statistically if there's like if it's the trillions of them surely some of them some of the pockets are close enough to get some of them happen to be close. Yeah. And close enough to see each other. And then once you see once you see something that is definitely complex life like if we see something. Yeah. We're probably going to be severe like intensely aggressively motivated to figure out what the hell that is and try to meet them. Well, it will be your first instinct to try to like at a generational level. Meet them or defend against them or it will be your instinct as a president of the United States. And the scientist. I don't know which hat you prefer in this question. Yeah, I think the question it's really hard. I will say like for example for us we have lots of primitive life forms on earth. Next to us we have all kinds of ants and everything else and we share space with them. And we are hesitant to impact on them and to we are trying to protect them by default because they are amazing interesting dynamical systems that took a long time to evolve and they are interesting and special and I don't know that you want to destroy that by default. And so I like complex dynamical systems that took a lot of time to evolve. I think I'd like to preserve it if I can afford to. And I'd like to think that the same would be true about the galactic resources and that they would think that we're kind of incredible interesting story that took time. It took a few billion years to unravel and you don't want to just destroy it. I could see two aliens talking about earth right now and saying I'm a big fan of complex dynamical systems. So I think it's with a value to preserve these and we'll basically are a video game they watch or show a TV show that they watch. Yeah, I think you would need like a very good reason I think to destroy it. Like why don't we destroy these ant farms and so on? It's because we're not actually like really indirect competition with them right now. We do it accidentally and so on but there's plenty of resources. And so why would you destroy something that is so interesting and precious? Well, from a scientific perspective, you might probe it. You might interact with it lately. Exactly. You might want to learn something from it. So I wonder there's could be certain physical phenomena that we think is a physical phenomena but it's actually interacting with us to like poke the finger and see what happens. I think it should be very interesting to scientists. Other alien scientists what happened here. And you know, it's a what we're seeing today as a snapshot basically. It's a result of a huge amount of computation over like billion years or something like that. So it could have been initiated by aliens. This could be a computer running a program like when okay, if you had the power to do this, when you okay, for sure, at least I would, I would pick a earth-like planet that has the conditions based my understanding of the chemistry prerequisites for life. And I would see it with life and run it. Right? Like, yeah, when you 100% do that and observe it and then protect, I mean, that's not just the hell of a good TV show. It's a good scientific experiment. And it's physical simulation, right? Maybe the evolution is the most like actually running it is the most efficient way to understand computation or to compute stuff. For understand life or you know, what life looks like and what branches it can take. It does make me kind of feel weird there were part of a science experiment but maybe it's everything's a science experiment. Does that change anything for us? For a science experiment? I don't know. Two descendants of Apes talking about being inside of a science experiment. I'm suspicious of this idea of like a deliberate pens premiere as you described it. And I don't see a divine intervention in some way in the historical record right now. I do feel like the story in these books like Nikolai's books and so on sort of makes sense and it makes sense how life arose on earth uniquely. And yeah, I don't need a, I mean, I don't need to reach for more exotic explanations right now. Sure, but NPCs inside a video game don't don't observe any divine intervention either. We might just be all NPCs running a kind of code. Maybe eventually they will. Currently, NPCs are really dumb but once they're running GPs, maybe they will be like, hey, this is really suspicious with the hell. So you famously tweeted, it looks like if you bombard earth with photons for a while, you can emit a roaster. So if like an Hitchhiker's guide to the galaxy, we would summarize the story of earth. So in that book it's mostly harmless. What do you think is all the possible stories, like a paragraph long or a sentence long, that earth could be summarized as? At once it's done, it's computation. So like all the possible full, if earth is a book, right? Yeah. Probably there has to be an ending. I mean, there's going to be an end to earth and it could end in all kinds of ways. It can end soon. It can end later. What do you think are the possible stories? Well, definitely there seems to be, yeah, you're sort of, it's pretty incredible that the self-replicating systems will basically arise from the dynamics and then they perpetuate themselves and become more complex and eventually become conscious and build a society. And I kind of feel like in some sense it's kind of like a deterministic wave that kind of just happens on any sufficiently well-arranged system like earth. And so I kind of feel like there's a certain sense of inevitability in it. And it's really beautiful. And it ends somehow, right? So it's a chemically diverse environment where complex dynamical systems can evolve and become more further further complex. But then there's a certain, what is it? There's certain terminating conditions. Yeah, I don't know what the terminating conditions are, but definitely there's a trend line of something. And we're part of that story. And like, where does that, where does it go? So, you know, we're famously described often as a biological bootloader for AIs. And that's because humans, I mean, we're an incredible biological system and we're capable of computation and, you know, and love and so on. But we're extremely inefficient as well. Like we're talking to each other through audio. It's just kind of embarrassing, honestly, that we're manipulating like seven symbols, uh, serially, we're using vocal chords. It's all happening over like multiple seconds. It's just like kind of embarrassing when you step down to the frequencies at which computers operate or are able to operate on. And so basically, it does seem like, um, synthetic intelligences are kind of like the next stage of development. And, um, I don't know where it leads to like at some point, I suspect the universe is some kind of a puzzle. And these synthetic AIs will uncover that puzzle and solve it. And then what happens after, right? Like what? Because if you just like fast forward earth, many billions of years, it's like, it's quiet. And then it's like, to the terminal, you see like city lights and stuff like that. And then what happens at like at the end, like, is it like a pool? Or is it like a calming? Is it explosion? Is it like earth like open like a giant? Because you said, admit roasters like we'll start emitting like like a giant number of like satellites. Yes. It's some kind of a crazy explosion. And we're living, we're like, we're stepping through a explosion. And we're like living day to day and it doesn't look like it. But it's actually, if you, I saw a very cool animation of earth, uh, and life on earth and basically nothing happens for a long time. And then the last like two seconds, like basically cities and everything. And the lower orbit just gets cluttered. And just the whole thing happens in the last two seconds. And you're like, this is exploding. This is a statement explosion. So if you play, yeah, yeah, if you play at a normal speed, yeah, it'll just look like an explosion. It's a firecracker. We're living in a firecracker where it's going to start emitting all kinds of interesting things. Yeah. And then the explosion doesn't, it might actually look like a little explosion with lights and fire and energy emitted, all that kind of stuff. But when you look in the inside, the details of the explosion, there's actual complexity happening where there's like, yeah, human life or some kind of life. We hope it's another destructive firecracker. It's kind of like a constructive firecracker. All right. So given that, I think, uh, hilarious. It's going to be really interesting to think about like what the puzzle of the universe is. Did the creator of the universe give us a message? Like for example, in the book contact, um, Carl Sagan, uh, there's a message for humanity, for any civilization in the digits in the expansion of pie and base 11 eventually. We're just kind of interesting thought. Maybe, maybe we're supposed to be giving a message to our creator. Maybe we're supposed to somehow create some kind of a quantum mechanical system that alerts them to our intelligent presence here. Because if you think about it from their perspective, it's just say like quantum field theory, massive like cellular atomic bomb like thing. And like, how do you even notice that we exist? You might not even be able to pick us up in that simulation. And so how do you, uh, how do you prove that you exist, uh, that you're intelligent and that you're part of the universe? So this is like a touring test for intelligence from Earth. Yeah. So creators, uh, I mean, maybe this is like trying to complete the next origin and sense. This is a complicated way of that. Like Earth is just is basically sending a message back. Yeah. The puzzle is basically like alerting the creator that we exist. Yeah. Uh, or maybe the puzzle is just to, uh, just break out of the system and just, uh, you know, stick it to the creator in some way. Yeah. Basically, like if you're playing a video game, you can, um, you can somehow find an exploit and find a way to execute on the host machine, uh, in the arbitrary code. Uh, there's some, uh, for example, I believe someone got a Mario, a game of Mario to play pong just by, um, exploiting it and then, um, creating a, basically writing, writing code and being able to execute arbitrary code in the game. And so maybe we should be, maybe that's the puzzle is that we should be, um, uh, find a way to exploit it. So, so I think like some of these synthetic ares will eventually find the universe to be some kind of a puzzle and then solve it in some way. And that's kind of like the end game somehow. Do you often think about it as, uh, as a simulation? So, uh, as, or the universe being a kind of computation that has, might have bugs and exploits. Yes. Yeah. I think so. I think it's possible that physics has exploits and we should be trying to find them, uh, arranging some kind of a crazy quantum mechanical system that somehow gives you buffer overflow, uh, somehow gives you a rounding error in the floating point. Uh, uh, uh, yeah. That's right. And like more and more sophisticated exploits that those are jokes, but that could be actually, yeah, we'll find some way to extract infinite energy. Uh, for example, when you train reinforcement learning agents, um, in physical simulations and you ask them to say run quickly on the flat ground, they'll end up doing all kinds of like weird things, um, in part of that optimization, right? They'll get on their back leg and they'll slide across the floor. And it's because the optimization, um, the enforcement learning optimization on that agent has figured out a way to extract infinite energy from the friction forces and, um, basically their poor implementation and they found a way to generate infinite energy and just slide across the surface. And it's not what you expected. It's just a, it's sort of like a, perverse solution. And so maybe we can find something like that. Maybe we can be that little dog in this physical simulation. The, like, the cracks or escapes the intended consequences of the physics that universe came up with. Yeah. We'll figure out some kind of shortcut to some weirdness. Yeah. And then, man, you see the problem with that weirdness is the first person to discover the weirdness, like sliding in the back legs. That's all we're going to do. Yeah. It's very quickly become everybody does that thing. So like the paperclip maximizer is a ridiculous idea, but that very well, you know, could be what then we'll just, we'll just all switch that because it's so fun. Well, no person will discover it, I think, by the way, I think it's going to have to be, uh, some kind of a super intelligent AGI of a Thorpe generation. Like we're building the first generation AGI. And you know, third generation. Yeah. So the bootloader for an AI, that AI will be a bootloader for another AI. Yeah. And then there's no way for us to interest back like what that would have been. I think it's very likely that these things, for example, like say you have these AGIs, it's very likely that, for example, they will be completely inert. I like these kinds of sci-fi books sometimes where these things are just completely inert. They don't interact with anything. And I find that kind of beautiful because they probably, they've probably figured out the meta game of the universe in some way, potentially. They're, they're doing something completely beyond our imagination. And they don't interact with simple chemical life forms. Why would you do that? So I find those kinds of ideas compelling. What's their source of fun? What are they, what are they doing? What's the source of pleasure? Well, probably solving in the universe. But inert, so can you define what it means in nerds? So they escape the actual physics. They will behave in some very like strange way to us because they're beyond, they're playing the meta game. And the meta game is probably say like arranging quantum mechanical systems in some very weird ways to extract infinite energy, solve the digital expansion of pie to whatever amount they will build their own like little fusion reactors or something crazy. Like they're doing something beyond comprehension. And not understandable to us and actually brilliant under the hood. What if quantum mechanics itself is the system and we're just thinking it's physics. But we're really parasites on, on not parasite, we're not really hurting physics. We're just living on this organisms. This organism and we're like trying to understand it, but really it is an organism. And with a deep, deep intelligence, maybe physics itself is the organism that's doing the super interesting thing. And we're just like one little thing, yeah, and sitting on top of it, trying to get energy from it. We're just kind of like these particles in the wave that I feel like is mostly deterministic and takes universe from some kind of a big bang to some kind of a super intelligent replicator, some kind of a stable point in the universe given these laws of physics. You don't think, as Einstein said, God doesn't play dice. So you think it's mostly deterministic. There's no randomness in the thing. I think as a deterministic, there's tons of, well, I'm I'm going to be careful with randomness pseudo random. Yeah, I don't like random. I think maybe the laws of physics are deterministic. Yeah, I think they're deterministic. He's just got really uncomfortable with the question. Do you have anxiety about whether the universe is random or not? There's no randomness. You say you like goodwill hunting. It's not your fault, Andre. It's not your fault, man. So you don't like randomness. Yeah, I think it's unsettling. I think it's a deterministic system. I think that things that look random, like say the collapse of the wave function, etc. I think they're actually deterministic, just entanglement and so on. And some kind of a multi-verse theory, something, something. Okay, so why does it feel like we have a free will? Like if I raise this hand, I chose to do this now. What that doesn't feel like a deterministic thing. It feels like I'm making a choice. It feels like it. Okay, so it's all feelings. It's just feelings. Yeah. So when RLA agent does make any choice, is that it's not really make any choice. The choice is already there. Yeah, you're interpreting the choice and you're creating a narrative for having made it. Yeah, and now we're talking about the narrative. It's very meta. Looking back, what is the most beautiful or surprising idea in deep learning or AI in general, that you've come across. You've seen this field explode and grow in interesting ways. Just what cool idea is like we made you sit back and go. Small, bigger, small. Well, the one that I've been thinking about recently, the most probably is the the transformer architecture. So basically, neural networks have a lot of architectures that where trendy have come and gone for different sensor modalities like for vision, audio, text. You would process them with different looking neural nets. And recently, we've seen this convergence towards one architecture, the transformer. And you can feed it video or you can feed it, images or speech or text and it just gobbles it up. And it's kind of like a bit of a general purpose computer. There is also trainable and very efficient to run in our hardware. And so this paper came out in 2016, I want to say attention is all you need. Detention is all you need. You could have said the paper title in retrospect that it wasn't it didn't foresee the bigness of the impact that it was going to have. Yeah, I'm not sure if the authors were aware of the impact that that paper would go on to have probably they weren't. But I think they were aware of some of the motivations and design decisions behind the transformer and they chose not to, I think, expand on it in that way in the paper. And so I think they had an idea that there was more than just the surface of just like over just doing translation and here's a better architecture. You're not just doing translation. This is like a really cool, differentiable, optimizable, efficient computer that you've proposed. And maybe they didn't have all of that foresight, but I think it's really interesting. Isn't it funny? Sorry to interrupt that title is memeable that they went for such a profound idea they went with the I don't think anyone used that kind of title before, right? Attention is all you need. Yeah, it's like a meme or something. Yeah, it's not funny that one. Like maybe if it was a more serious title, we don't have the impact. Honestly, yeah, there's an element of me that honestly agrees with you and prefers it this way. Yes. If it was too grand, it would overpromise and then underdeveloper potentially. So you want to just meme your way to greatness. That should be a t-shirt. So you tweeted the transformers and magnificent neural network architecture because it is a general purpose, differentiable computer. It is simultaneously expressive in the forward pass, optimizable via back propagation gradient descent and efficient high parallelism compute graph. Can you discuss some of those details expressed of optimizable, efficient, yeah, for memory or in general, whatever comes to your heart? You want to have a general purpose computer that you can train on arbitrary problems. Like say the task of next work prediction or detecting if there's a cat in a image or something like that. And you want to train this computer so you want to set its weights. And I think there's a number of design criteria that sort of overlap in the transformer simultaneously that made it very successful. And I think the author is where kind of deliberately trying to make this really powerful architecture. And so in a, basically it's very powerful in the forward pass because it's able to express very general computation as sort of something that looks like message passing. You have nodes and the old store vectors. And these nodes get to basically look at each other and it's each other's vectors. And they get to communicate. And basically nodes get to broadcast, hey, I'm looking for certain things. And then other nodes get to broadcast, hey, these are the things I have. Those are the keys in the values. So it's not just attention. Yeah, exactly. Chessformer is much more than just the attention component that's got many pieces architectural that went into it. The residual connection of the weights arranged. There's a multi-layer perceptron and they're the weights stacked and so on. But basically there's a message passing scheme where nodes get to look at each other, decide what's interesting, and then update each other. And so I think the, when you get to the details of it, I think it's a very expressive function. So it can express lots of different types of algorithms in forward pass. Not only that, but the weights designed with the residual connections, lane normalizations, the softmatics, attention and everything. It's also optimizable. This is a really big deal because there's lots of computers. There are powerful that you can't optimize. Or they are not easy to optimize using the techniques that we have, which is backpropocation and gradient descent. These are our first order methods, very simple optimizers, really. And so you also needed to be optimizable. And then lastly, you wanted to run efficiently in our hardware. Our hardware is a massive throughput machine, like GPUs. They prefer lots of parallelism. So you don't want to do lots of sequential operations. You want to do a lot of operations seriously. And the transformer is designed with that in mind as well. And so it's designed for our hardware and is designed to both be very expressive in a forward pass, but also very optimizable in the backward pass. And you said that the residual connection support of kind of ability to learn short algorithms fast and first and then gradually extend them longer during training. Yeah. What's the idea of learning short algorithms? Right. Think of it as a, so basically a transformer is a series of blocks, right. And these blocks have attention and a little more to the upper section. And so you go off into a block and you come back to this residual pathway. And then you go off and you come back. And then you have a number of layers arranged sequentially. And so the way to look at it, I think, is because of the residual pathway in the backward pass, the gradients sort of flow allowing it uninterrupted because addition distributes the gradient equally to all of its branches. So the gradient from the supervision at the top just floats directly to the first layer. And the all the residual connections are arranged so that in the beginning and during initialization, they contribute nothing to the residual pathway. So what it kind of looks like is imagine the transformer is kind of like a Python function, like a death. And you get to do various kinds of like lines of code. So you have a hundred layers deep transformer. Typically they would be much shorter, say 20. So you have 20 lines of code and you can do something in them. And so think of during the optimization, basically what it looks like is first you optimize the first line of code and then the second line of code can kick in and the third line of code can kick in. And I kind of feel like because of the residual pathway and the dynamics of the optimization, you can sort of learn a very short algorithm that gets the approximate answer. But then the other layers can sort of kick in and start to create a contribution. And at the end of it, you're optimizing over an algorithm that is 20 lines of code. Except these lines of code are very complex because this is an entire block of a transformer. You can do a lot in there. Well, it's really interesting is that this transformer architecture actually has been a remarkably resilient. Basically, the transformer that came out in 2016 is the transformer you would use today, except you reshuffle some delayer norms. The delayer normalizations have been reshuffle to a pre-norm formulation. And so it's been remarkably stable, but there's a lot of bells and whistles that people have attached on it and try to improve it. I do think that basically it's a big step in simultaneously optimizing for lots of properties of a desirable neural network architecture. And I think people have been trying to change it, but it's proven remarkably resilient. But I do think that there should be even better architectures potentially. But it's you admire the resilience here. There's something profound about this architecture that at least reshuffle. So maybe we can everything can be turned into a problem that transformers can solve. Currently, definitely looks like the transformer is taking over AI and you can feed basically arbitrary problems into it. And it's a general, the Frenchable computer and it's extremely powerful. And this conversion in AI has been really interesting to watch for me personally. What else do you think could be discovered here about transformers? Like what's the surprising thing? Or is it a stable, I want a stable place. Is there something interesting where my discover about transformers? Like a Ha moment, maybe has to do with memory, maybe knowledge representation, that kind of stuff. Definitely the zeitgeist today is just pushing, like basically right now the zeitgeist is do not touch the transformer, touch everything else. So people are scaling up the data sets, making them much, much bigger. They're working on the evaluation, making the evaluation much, much bigger. And they're basically keeping the architecture unchanged. And that's how we've, that's the last five years of progress in AI kind of. What do you think about one flavor of it, which is language models? Have you been surprised? Has your sort of imagination been captivated by you mentioned GPD and all the big and big and bigger language models? And what are the limits of those models do you think? So just the task of natural language. Basically the way GPD is trained, right? As you just download a massive amount of text data from the internet and you try to predict the next word in a sequence, roughly speaking. You're predicting a little work chunks, but roughly speaking, that's it. And what's been really interesting to watch is basically it's a language model. Language models have actually existed for a very long time. There's papers on language modeling from 2003, even earlier. Can you explain in that case what language model is? Yeah, so language model just basically the rough idea is just predicting the next word in a sequence, roughly speaking. So there's a paper from, for example, Ben Geo and the team from 2003, where for the first time they were using a neural network to take say like three or five words and predict the next word. And they're doing this on much smaller datasets and the neural net is not a transformer. It's a multi-air perceptron, but it's the first time that a neural network has been applied in that setting. But even before neural networks, there were language models, except they were using N-gram models. So N-gram models are just a count-based models. So if you try to take two words and predict the third one, you just count up how many times you've seen any two word combinations and what came next. And what you predict as coming next is just what you've seen the most of in the training set. And so language modeling has been around for a long time. Neural networks have done language modeling for a long time. So really what's new or interesting or exciting is just realizing that when you scale it up with powerful enough neural net transformer, you have all these emerging properties where basically what happens is if you have a large enough dataset of text, you are in the task of predicting the next word, you are multitasking a huge amount of different kinds of problems. You are multitasking understanding of chemistry, physics, human nature. Lots of things are sort of clustered in that objective. It's a very simple objective, but actually you have to understand a lot about the world to make that prediction. You just said the you word understanding. Are you in terms of chemistry and physics and so on? What do you feel like is doing? Is it searching for the right context? What is the actual process happening here? Yeah, so basically it gets a thousand words and is trying to predict a thousand at first. And in order to do that very, very well over the entire dataset available on the internet, you actually have to basically kind of understand the context of what's going on in there. It's a sufficiently hard problem that if you have a powerful enough computer like a transformer, you end up with interesting solutions. You can ask it to do all kinds of things. It shows a lot of emerging properties like in context learning. That was the big deal with GPD and the original paper when they published it is that you can just sort of prompt it in various ways and ask it to do various things and it will just kind of complete the sentence. But in a process of just completing the sentence, it's actually solving all kinds of really interesting problems that we care about. Do you think it's doing something like understanding? When we use the word understanding for us humans, I think it's doing some understanding. In its way, it understands I think a lot about the world and it has to in order to predict the next word in the sequence. So let's train on the data from the internet. What do you think about this this approach in terms of datasets of using data from the internet? Do you think the internet has enough structured data to teach AI about human civilization? Yes, so I think the internet has a huge amount of data. I'm not sure if it's a complete enough set. I don't know that text is enough for having a sufficiently powerful AGI as an outcome. Of course, there is audio and video and images and all that kind of stuff. Yeah, so text by itself, I'm a little bit suspicious about. There's a ton of things we don't put in text in writing just because they're obvious to us about how the world works and the physics of it and the things fall. We don't put that stuff in text because why would you? We share that understanding. And so text is communication medium between humans and it's not a all-encompassing medium of knowledge about the world. But as you pointed out, we do have video and we have images and we have audio. And so I think that definitely helps a lot. But we haven't trained models sufficiently across all of those modalities yet. So I think that's what a lot of people are interested in. But I wonder what that shared understanding of what we may call common sense has to be learned inferred in order to complete the sentence correctly. So maybe the fact that it's implied on the internet, the model's going to have to learn that not by reading about it, by inferring it in the representation. So like common sense, just like we, I don't think we learn common sense. Like nobody says, tells us explicitly, we just figure it all out by interacting with the world. And so here's a model of reading about the way people interact with the world. It might have to infer that. I wonder. Yeah. You briefly worked on a project called World of Bits training in our RL system to take actions on the internet versus just consuming the internet. Like you talked about, do you think there's a future for that kind of system interacting with the internet to help the learning? Yes, I think that's probably the final frontier for a lot of these models. Because as you mentioned, when I was at OpenAI, I was working on this project World of Bits. And basically, it was the idea of giving neural networks access to a keyboard and a mouse. And the idea is possibly go wrong. So basically, you, you perceive the input of the screen pixels and basically the state of the computer is sort of visualized for human consumption in images of the web browser and stuff like that. And then you give the neural or the ability to press keyboards and use the mouse. And we're trying to get it to, for example, complete bookings and interact with user interfaces. And would you learn from that experience? Like, what was some fun stuff? This is super cool idea. Yeah. I mean, it's like, yeah, I mean, the step between observer to actor is a super fascinating step. Yeah. Well, it's the universal interface in the digital realm, I would say. And there's a universal interface in like the physical realm, which in my mind is a humanoid form factor kind of thing. We can later talk about optimists and so on. But I feel like there's a, they're kind of like similar philosophy in some way, where the human, the world, the physical world is designed for the human form. And the digital world is designed for the human form of seeing the screen and using keyword, not keyboard and mouse. And so it's the universal, universal interface that can basically command the digital infrastructure we've built up for ourselves. And so it feels like a very powerful interface to, to command and to build on top of. Now, to your question is to like what I learned from that? It's interesting because the world of bits was basically too early, I think at OpenAI at the time. This is around 2015 or so. And the zeitgeist at that time was very different in AI from the zeitgeist today. At the time, everyone was super excited about reinforcement learning from scratch. This is the time of the Atari paper where neural networks were playing Atari games and beating humans in some cases, AlphaGo and so on. So everyone was very excited about training, training neural networks from scratch using reinforcement learning directly. It turns out that reinforcement learning is extremely an efficient way of training neural networks because you're taking all these actions and all these observations and you get some sparse rewards once in a while. So you do all this stuff based on all these inputs. And once in a while, you're like told you did a good thing, you did a bad thing. And it's just an extremely hard problem when you can't learn from that. You can burn a forest and you can sort of brute force through it. And we saw that I think with Go and Dota and so on, and does work. But it's extremely inefficient and not how you want to approach problems practically speaking. And so that's the approach that at the time we also took to world of bits. We would have an agent initialize randomly, so with keyboard mash and mouse mash and try to make a booking. And it's just like revealed the insanity of that approach very quickly, where you have to stumble by the correct booking in order to get a reward of you did it correctly. And you're never going to stumble by it by chance at random. So even with a simple web interface, there's too many options. There's just too many options. And it's two sparse reward signal. And you're starting from scratch at the time. And so you don't know how to read. You don't understand pictures, images, buttons. You don't understand what it needs to like make a booking. But now what's happened is it is time to revisit that and opening eyes is interesting in this. Companies like ADEPT are interested in this and so on. And the idea is coming back because the interface is very powerful. But now you're not training an agent from scratch. You are taking the GPT as an initialization. So GPT is pre-trained on all of text. And it understands what's a booking. It understands what's a submit. It understands quite a bit more. And so it already has those representations. They are very powerful. And that makes all of the training significantly more efficient and makes the problem intractable. Should the interaction be like the way humans see it, with the buttons and the language, or should be with the HTML JavaScript and the CSS? What do you think is the better? Today all of this interaction is mostly on the level of HTML, CSS and so on. That's done because of computational constraints. But I think ultimately everything is designed for a human visual consumption. And so at the end of the day, there's all the additional information is in the layout of the web page and what's next to it and what's our red background and all this kind of stuff and what it looks like visually. So I think that's the final frontier as we are taking in pixels and we're giving out the keyboard mouse commands. But I think it's in practical still today. Do you worry about bots on the internet? Given these ideas, given how exciting they are, do you worry about bots on Twitter being not the stupid bots that we see now with the crypto bots? But the bots that might be out there actually that we don't see that they're interacting interesting ways. So this kind of system feels like it should be able to pass the, I'm not a robot click button, whatever. Which do you actually understand how that test works? I don't quite like there's a check box or whatever that you click. It's presumably tracking mouse movement and the timing and so on. So exactly this kind of system we're talking about should be able to pass that. So yeah, what do you feel about bots that are language models plus have some interactability and are able to tweet and reply and so on. Do you worry about that world? Yeah, I think it's always been a bit of an arms race between sort of the attack and the defense. So the attack will get stronger but the defense will get stronger as well. Our ability to detect that. How do you defend? How do you detect? How do you know that your Carpate account on Twitter is human? How would you approach that? Like if people were claim, you know, how would you defend yourself in the court of law that I'm a human? This account is. Yeah, at some point I think it might be, I think the society will evolve a little bit like we might start signing digitally signing some of our correspondence or things that we create. Right now it's not necessary but maybe in the future it I do think that we are going towards a world where we share the digital space with AI's synthetic beings. Yeah, and they will get much better and they will share our digital realm and they'll eventually share our physical realm as well. It's much harder. But that's kind of like the world we're going towards and most of them will be benign and awful and some of them will be malicious and it's going to be an arms race trying to detect them. So I mean the worst isn't the AI's the worst is the AI is pretending to be human. So I don't know if it's always malicious. There's obviously a lot of malicious applications but it could also be, if I was an AI I would try very hard to pretend to be human because we're in a human world. I wouldn't get in your respect as an AI. I want to get some love and respect. I don't think the problem is intractable. People are thinking about the proof of personhood and we might start digitally signing our stuff and we might all end up having like, yeah, basically some solution for proof of personhood. It doesn't seem to me intractable. It's just something that we haven't had to do until now but I think once the need like really starts to emerge which is soon I think people will think about it much more. But that too will be a race because obviously you can probably spoof or fake the proof of personhood. So you have to try to figure out how to probably. I mean it's weird that we have like social security numbers and like passports and stuff. It seems like it's harder to fake stuff in the physical space but in the digital space it just feels like it's going to be very tricky. Very tricky to out because it seems to be pretty low cost to fake stuff. What are you going to put an AI in jail for like trying to use a fake personhood proof? I mean okay fine you'll put a lot of AI in jail but there'll be more AI's like exponentially more. The cost of creating bought is very low. Unless there's some kind of way to track accurately like you're not allowed to create any program without showing tying yourself to that program. Like any program that runs on the internet you'll be able to trace every single human program that was involved with that program. Yeah maybe you have to start declaring when you know we have to start drawing those boundaries and keeping track of okay what are digital entities versus human entities and what is the ownership of human entities and digital entities and something like that. I don't know but I think I'm optimistic that this is possible and in some sense we're currently in like the worst time of it because all these bots suddenly have become very capable but we don't have the fences yet built up the society but I think that doesn't seem to me intractable it's just something that we have to deal with. It seems weird that the Twitter bot like really crappy Twitter bots are so numerous. I guess. So I presume that the engineers at Twitter are very good. So it seems like what I would infer from that is it seems like a hard problem. They're probably catching... All right if I were to sort of steal man the case it's a hard problem and there's a huge cost to false positive to removing a post by somebody that's not a bot. That's a crazy very bad user experience so they're very cautious about removing. So maybe it's and maybe the bots are really good at learning what gets removed and not such that they can stay ahead of the removal process very quickly. My impression of it honestly is there's a lot of blowing fruit. I mean yeah just that's what I... It's not so hard. It's my impression of it. It's not so hard. But you have to... Yeah that's my impression as well but it feels like maybe you're seeing the the tip of the iceberg. Maybe the number of bots isn't like the trillions and you have to like just it's a constant assault of bots and yeah you yeah yeah I don't know. You have to steal man the case because the bots I'm seeing are pretty like obvious. I could write a few lines of code that catch these bots. I mean definitely there's a lot of blowing fruit but I will say I agree that if you are a sophisticated actor you could probably create a pretty good bot right now. You know using tools like GPTs because it's a language model. You can generate faces that look quite good now and you can do this at scale. And so I think yeah it's quite plausible and it's going to be hard to defend. There was a Google engineer that claimed that the Lambda was sentient. Do you think there's any inkling of truth to what he felt and more importantly to me at least do you think language models will achieve sentience or the illusion of sentience? Sunish. Yeah to me it's a little bit of a canary in a coal mine kind of moment honestly a little bit because so this engineer spoke to like a chat bot at Google and we can convince that this bot is sentient. Asked it's some existential philosophical question and it gave like reasonable answers and looked real and so on. So to me it's a he was he wasn't sufficiently trying to stress the system I think and exposing the truth of it as it is today but I think this will be increasingly harder over time. So yeah I think more and more people will basically become yeah I think more and more there will be more people like that over time as this gets better like form and emotional connection to an AI. Yeah perfectly plausible in my mind. I think these AI's are actually quite good at human connection human emotion. A ton of text on the internet is about humans and connection and love and so on. So I think they have a very good understanding in some in some sense of of how people speak to each other about this and they're very capable of creating a lot of that kind of text. There's a lot of like sci-fi from 50s and 60s that imagined AI's in a very different way. They are calculating coal Balkan-like machines. That's not what we're getting today. We're getting pretty emotional AI's that actually are very competent and capable of generating you know possible sounding text with respect to all of these topics. Yeah I'm really hopeful about AI systems that are like companions that help you grow, develop as a human being, help you maximize long term happiness. But I'm also very worried about AI systems that figure out from the internet that humans get attracted to drama. So these would just be like shit talking AI's. Did you hear it? They'll do gossip. They'll do they'll try to plant seeds of suspicion to other humans that you love and trust and just kind of mess with people. You know because that's going to get a lot of attention. So drama, maximize drama on the path to maximizing engagement and us humans will feed into that machine. Yeah. And get it'll be a giant drama shit storm. Yeah. So I'm worried about that. So it's the objective function really defines the way that humans of physician progresses with AI's in it. Yeah. I think right now at least today they are not sort of it's not correct to really think of them as goal seeking agents that want to do something. They have no long term memory or anything. It's literally a good approximation of it is you get a thousand words and you're trying to pretty good a thousand at first and then you continue feeding it in. And you are free to prompt it in whatever way you want. So in text, so you say okay you are a psychologist and you are very good and you love humans. And here's a conversation between you and another human, human calling something, you something. And then it just continues the pattern. And suddenly you're having a conversation with a fake psychologist who's trying to help you. And so it's still kind of like an enrollment rate tool. It is a, people can prompt it in arbitrary ways and it can create really incredible text. But it doesn't have long term goals over long periods of time. It doesn't try to, so it doesn't look that way right now. Yeah. But you can do short term goals that have long term effects. So if my prompting short term goal is to get Andrzej Kapatich respond to me on Twitter whenever. Like I think I might, that's the goal, but it might figure out that talking shit to you will be the best in a highly sophisticated interesting way. And then you build up a relationship when you respond once. And then it like over time it gets to not be sophisticated and just like just talk shit. And okay, maybe you won't get to Andrzej, but it might get to another celebrity and might get into other big accounts. And then it'll just, so with just that simple goal, get them to respond. Yeah. Maximize the probability of actual response. Yeah, I mean, you could prompt a powerful model like this with their, it's opinion about how to do any possible thing you're interested in. So they will check us. They're kind of on track to become these oracles. I could sort of think of it that way. They are oracles currently is just text, but they will have calculators, they will have access to Google search, they will have all kinds of gadgets and gizmos, they will be able to operate the internet and find different information. And yeah, in some sense, that's kind of like currently what it looks like in terms of the development. Do you think it'll be an improvement eventually over what Google is for access to human knowledge? Like it'll be a more effective search engine to access human knowledge. I think there's definitely scope in building a better search engine today. And I think Google, they have all the tools, all the people, they have everything they need, they have all the puzzle pieces, they have people training transformers at scale, they have all the data. It's just not obvious if they are capable as an organization to innovate on their search engine right now. And if they don't, someone else will, there's absolute scope for building a significantly better search engine built on these tools. It's so interesting. A large company where the search, there's already an infrastructure, it works as it brings out a lot of money. So where structuring inside a company is their motivation to pivot to say we're going to build a new search engine. That's hard. So it's usually going to come from a startup. That's that would be yeah, or some other more competent organization. So I don't know. So currently, for example, maybe Bing has another shot at it. You know, as an example, Microsoft, we're talking offline. I mean, I definitely, it's really interesting because search engines used to be about, okay, here's some query, here's, here's web pages that look like the stuff that you have, but you could just directly go to answer and then have supporting evidence. And these models basically, they've read all the texts and they've read all the web pages. And so sometimes when you see yourself going over to search results and sort of getting like a sense of like the average answer to whatever you're interested in, like that just directly comes out. You don't have to do that work. So they're kind of like, yeah, I think they have a way to this of distilling all that knowledge into like some level of insight, basically. Do you think of prompting as a kind of teaching and learning like this whole process, like another layer? You know, because maybe that's what humans are. We already have that background model and the world is prompting you. Yeah, exactly. I think the way we are programming these computers now, like GPDs, is converging to how you program humans. I mean, how do I program humans via prompt? I go to people and I prompt them to do things. I prompt them for information. And so natural language prompt is how we program humans. And we're starting to program computers directly in that interface. It's like pretty remarkable, honestly. So you've spoken a lot about the idea of software 2.0. All good ideas become like clichs so quickly like the terms. It's kind of hilarious. It's like I think M&M once said that like if he gets annoyed by a song he's written very quickly, that means it's going to be a big hit because it's too catchy. But can you describe this idea and how you're thinking about it has evolved over the months and years since you coined it? Yeah. Yes, I had a blog post on software 2.0. I think several years ago now. And the reason I wrote that post is because I kept, I kind of saw something remarkable happening in like software development and how a lot of code was being transitioned to be written not in sort of like C++ and so on, but it's written in the weights of a neural net. Basically just saying that neural nets are taken over software, the realm of software and taking more and more and more tasks. And at the time, I think not many people understood this deeply enough that this is a big deal, this is a big transition. Neural networks were seen as one of multiple classification algorithms you might use for your dataset problem on Kaggle. Like this is not that. This is a change in how we program computers. And I saw neural nets as this is going to take over. The way we program computers is going to change. It's not going to be people writing a software in C++ or something like that and directly programming the software. It's going to be accumulating training sets and data sets and crafting these objectives by which you train these neural nets. And at some point, there's going to be a compilation process from the datasets and the objective and the architecture specification into the binary, which is really just the neural net weights and the forward pass of the neural net. And then you can deploy that binary. And so I was talking about that sort of transition and that's what the post is about. And I saw this sort of play out in a lot of fields, you know, auto, auto, auto, auto, being one of them, but also just simple image classification. People thought originally, you know, in the 80s and so on that they would write the algorithm for detecting a dog in an image. And they had all these ideas about how the brain does it. And first we detect corners and then we detect lines and then we stitch them up. And they were like really going at it. They were like thinking about how they're going to write the algorithm. And this is not the way you build it. And there was a smooth transition where, okay, first we thought we were going to build everything. Then we were building the features, so like hog features and things like that that detect these little statistical patterns from image patches. And then there was a little bit of learning on top of it, like a support vector machine or binary classifier for cat versus dog and images on top of the features. So we wrote the features, but we trained the last layer sort of the classifier. And then people are like, actually let's not even design the features because we can't honestly, we're not very good at it. So let's also learn the features. And then you end up with basically a convolution on your own where you're learning most of it. You're just specifying the architecture. And the architecture has tons of filling blanks, which is all the knobs. And you let the optimization write most of it. And so this transition is happening across the industry everywhere. And suddenly we end up with a ton of code that is written in neural net weights. And I was just pointing out that the analogy is actually pretty strong. And we have a lot of developer environments for software 1.0, like we have IDEs, how you work with code, how you debug code, how you run code, how do you maintain code, we have GitHub. So I was trying to make those analogies in the URL. Like what is the GitHub software 2.0? Turns out it's something that looks like hugging face right now. And so I think some people took it seriously and built cool companies. And many people originally attacked the post. It actually was not built received when I wrote it. And I think maybe it has something to do with the title, but the post was not well received. And I think more people have been coming around to it over time. Yeah. So you were the director of AI at Tesla, where I think this idea was really implemented at scale, which is how you have engineering teams doing software 2.0. So can you sort of linger on that idea of I think we're in the really early stages of everything you just said, which is like GitHub IDEs, like how do we build engineering teams that that work in software 2.0 systems and the data collection and the data annotation, which is all part of that software 2.0. Like what do you think is the task of programming software 2.0? Is it debugging in the space of hyper parameters or is it also debugging in the space of data? Yeah. The way by which you program the computer and influence its algorithm is not by writing the commands yourself. You're changing mostly the data set. You're changing the loss functions of what the neural net is trying to do, how it's trying to predict things, but basically the data sets and the architecture of the neural net. And so in the case of the autopilot, a lot of the data sets had to do with, for example, detection of objects and lane line markings and traffic lights and so on. So you accumulate massive data sets of, here's an example, here's the desired label. And then here's roughly what the algorithm should look like, and that's a completion on your own net. So the specification of the architecture is like a hint as to what the algorithm should roughly look like. And then the fill in the blanks process of optimization is the training process. 